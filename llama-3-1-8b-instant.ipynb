{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12508883,"sourceType":"datasetVersion","datasetId":7895256}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install groq langchain-community pdfplumber","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T14:47:01.528397Z","iopub.execute_input":"2025-07-18T14:47:01.528926Z","iopub.status.idle":"2025-07-18T14:47:12.353271Z","shell.execute_reply.started":"2025-07-18T14:47:01.528901Z","shell.execute_reply":"2025-07-18T14:47:12.352547Z"}},"outputs":[{"name":"stdout","text":"Collecting groq\n  Downloading groq-0.30.0-py3-none-any.whl.metadata (16 kB)\nCollecting langchain-community\n  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\nCollecting pdfplumber\n  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m411.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\nRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.7)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\nRequirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.14.0)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.66)\nRequirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.4)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.12.13)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\nRequirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (1.26.4)\nCollecting pdfminer.six==20250506 (from pdfplumber)\n  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\nCollecting pypdfium2>=4.18.0 (from pdfplumber)\n  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\nRequirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (44.0.3)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\nRequirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\nCollecting packaging<25,>=23.2 (from langchain-core<1.0.0,>=0.3.66->langchain-community)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.5.0)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\nRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain-community) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain-community) (2024.2.0)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\nDownloading groq-0.30.0-py3-none-any.whl (131 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\nDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\nInstalling collected packages: python-dotenv, pypdfium2, packaging, httpx-sse, pydantic-settings, pdfminer.six, groq, pdfplumber, langchain-community\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed groq-0.30.0 httpx-sse-0.4.1 langchain-community-0.3.27 packaging-24.2 pdfminer.six-20250506 pdfplumber-0.11.7 pydantic-settings-2.10.1 pypdfium2-4.30.1 python-dotenv-1.1.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import json\nfrom groq import Groq\nfrom langchain_community.document_loaders import PDFPlumberLoader\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T14:48:04.858820Z","iopub.execute_input":"2025-07-18T14:48:04.859463Z","iopub.status.idle":"2025-07-18T14:48:06.270793Z","shell.execute_reply.started":"2025-07-18T14:48:04.859432Z","shell.execute_reply":"2025-07-18T14:48:06.270016Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"GROQ_API_KEY\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T14:48:09.402403Z","iopub.execute_input":"2025-07-18T14:48:09.402797Z","iopub.status.idle":"2025-07-18T14:48:09.654864Z","shell.execute_reply.started":"2025-07-18T14:48:09.402776Z","shell.execute_reply":"2025-07-18T14:48:09.654284Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"system_prompt = \"\"\"You are an AI assistant designed to extract structured resume data.\n                   Always respond with a strictly valid JSON object. Use `null` for missing values,\n                   ensuring compliance with JSON standards. Do not include explanations,\n                   comments, or any additional text outside the JSON structure.\n                \"\"\"\n\n\n\nhuman_prompt = \"\"\"\n             **Task:** Extract key information from the following resume text.\n\n            **Resume Text:**\n            {context}\n\n            **Instructions:**\n            Please extract the following information and format it in a clear structure:\n\n            1. **Contact Information:**\n            - Name:\n            - Email:\n            - Phone Number:\n            - Website/Portfolio/LinkedIn:\n            - Github Profile:\n\n            2. **Education:**\n            - Institution Name:\n            - Degree:\n            - Graduation Date:\n\n            3. **Experience:**\n            - Job Title:\n            - Company Name:\n            - Location:\n            - Dates of Employment:\n            - Description:\n\n            5. **Skills:**\n            - Skills:\n\n            **Question:**\n            Extract this information as a structured and valid JSON object. Use `null` for missing or unavailable valuesDo not include explanations,\n                   comments, or any additional text outside the JSON structure.\n        \"\"\"\n\n\n\n# Initialize Groq client once\nclient = Groq(api_key=secret_value_0) \n\n# Your system and human prompts (define these before the loop)\nsystem_prompt = system_prompt\nhuman_prompt = human_prompt\n\ndef clean_resume_data(json_data):\n    \"\"\"Clean and standardize the resume JSON data\"\"\"\n    # 1. Remove duplicate skills (case insensitive)\n    if 'skills' in json_data and isinstance(json_data['skills'], list):\n        seen_skills = set()\n        unique_skills = []\n        for skill in json_data['skills']:\n            lower_skill = skill.strip().lower()\n            if lower_skill not in seen_skills:\n                seen_skills.add(lower_skill)\n                unique_skills.append(skill.strip())\n        json_data['skills'] = unique_skills\n    \n    # 2. Standardize website/portfolio fields\n    website_aliases = [\n        'website', 'portfolio', 'linkedin', \n        'Website', 'Portfolio', 'LinkedIn',\n        'personal_website', 'webpage'\n    ]\n    \n    # Find the first existing website-related field\n    website_field = None\n    website_value = None\n    for field in website_aliases:\n        if field in json_data:\n            website_field = field\n            website_value = json_data[field]\n            break\n    \n    # Standardize to 'website' if we found a value\n    if website_field and website_value:\n        # Remove all website-related fields\n        for field in website_aliases:\n            if field in json_data:\n                del json_data[field]\n        # Add the standardized field\n        json_data['website'] = website_value.strip()\n    \n    return json_data\n\ndef extract_text_pdf(pdf_path):\n    loader = PDFPlumberLoader(pdf_path)\n    docs = loader.load()\n    text = ''\n    for doc in docs:\n        text += doc.page_content\n    return text\n\ndef process_resume(pdf_path, index):\n    # Extract text from PDF\n    context = extract_text_pdf(pdf_path)\n    \n    # Call LLM API\n    completion = client.chat.completions.create(\n        # model=\"meta-llama/llama-4-scout-17b-16e-instruct\"\n        model=\"llama-3.1-8b-instant\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": human_prompt.format(context=context)\n            }\n        ],\n        temperature=1,\n        max_completion_tokens=1024,\n        top_p=1,\n        stream=True,\n        stop=None,\n    )\n    \n    # Process the streamed output\n    output = \"\"\n    for chunk in completion:\n        content = chunk.choices[0].delta.content or \"\"\n        print(content, end=\"\")\n        output += content\n    \n    # Extract JSON content\n    json_start = output.find('{')\n    json_end = output.rfind('}') + 1\n    json_content = output[json_start:json_end]\n    \n    # Save JSON to file\n    try:\n        json_data = json.loads(json_content)\n        \n        # Clean and standardize the data\n        cleaned_data = clean_resume_data(json_data)\n        \n        output_filename = f\"Resume_data_pdf_{index}.json\"\n        with open(output_filename, 'w') as f:\n            json.dump(cleaned_data, f, indent=2)\n        print(f\"\\nJSON saved successfully to {output_filename}\")\n        return True\n    except json.JSONDecodeError as e:\n        print(f\"Error parsing JSON for {pdf_path}: {e}\")\n        # Save raw output for debugging\n        with open(f\"Raw_output_pdf_{index}.json\", 'w') as f:\n            f.write(output)\n        return False\n\n\n\n# # # Correct paths (/) # working\n# pdf_path1 = \"/kaggle/input/resume-17b/BHARATH_RESUME-complete.pdf\"\n# pdf_path2 = \"/kaggle/input/resume-17b/Bharathkumar-Parunandula-Resume.pdf\"   \n# pdf_path3 = \"/kaggle/input/retsdts/BHARATH_RESUME-DS-FINAL.pdf\"   \n# # pdf_path4 = \"PARSER-APP INPUT/50328713.rank4.pdf\"\n# # pdf_path5 = \"PARSER-APP INPUT/17823436.rank5.pdf\"\n\n\n# import os\n\npdf_paths = []\nfor dirname, _, filenames in os.walk('/kaggle/input/testlres'):\n    for filename in filenames:\n        if filename.lower().endswith('.pdf'):\n            pdf_paths.append(os.path.join(dirname, filename))\n\nprint(\"Found PDF files:\")\nfor path in pdf_paths:\n    print(path)\n\n# Process each PDF separately\n# pdf_paths = [pdf_path1,pdf_path2,pdf_path3]\nfor i, pdf_path in enumerate(pdf_paths, start=1):\n    print(f\"\\nProcessing resume {i}: {pdf_path}\")\n    process_resume(pdf_path, i)\n    print(f\"Completed processing {pdf_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T14:49:38.057380Z","iopub.execute_input":"2025-07-18T14:49:38.058029Z","iopub.status.idle":"2025-07-18T14:52:47.479325Z","shell.execute_reply.started":"2025-07-18T14:49:38.057993Z","shell.execute_reply":"2025-07-18T14:52:47.478316Z"}},"outputs":[{"name":"stdout","text":"Found PDF files:\n/kaggle/input/testlres/cover letter.pdf\n/kaggle/input/testlres/BHARATH_RESUME-DS_final.pdf\n/kaggle/input/testlres/BHARATH-RESUME V3.pdf\n/kaggle/input/testlres/BHARATH_RESUME-DS-FINAL.pdf\n/kaggle/input/testlres/Bharathkumar-Parunandula-Resume.pdf\n/kaggle/input/testlres/Bharathkumar-Parunandula-FlowCV-Resume-20250704.pdf\n/kaggle/input/testlres/BHARATH_RESUME-DS-3.pdf\n/kaggle/input/testlres/Bharathkumar-Parunandula-resume-final.pdf\n/kaggle/input/testlres/Bharathkumar-Parunandula-FlowCV-Resume-20250619.pdf\n/kaggle/input/testlres/BHARATH-RESUME-DS.pdf\n/kaggle/input/testlres/BHARATH_RESUME-DS2.pdf\n/kaggle/input/testlres/BHARATH_RESUME-complete.pdf\n/kaggle/input/testlres/BHARATH-RESUME V2.pdf\n/kaggle/input/testlres/cover1.pdf\n/kaggle/input/testlres/Bharathkumar Original Resume 2.pdf\n/kaggle/input/testlres/BHARATH-RESUME.pdf\n/kaggle/input/testlres/BHARATH_RESUMEDSv4.pdf\n/kaggle/input/testlres/BHARATH_RESUMEDS.pdf\n\nProcessing resume 1: /kaggle/input/testlres/cover letter.pdf\n```json\n{\n  \"contactInformation\": {\n    \"name\": \"Bharathkumar Parunandula\",\n    \"email\": null,\n    \"phoneNumber\": null,\n    \"website\": null,\n    \"portfolio\": null,\n    \"linkedin\": null,\n    \"githubProfile\": null\n  },\n  \"education\": {\n    \"institutionName\": null,\n    \"degree\": null,\n    \"graduationDate\": null\n  },\n  \"experience\": []\n}\n```\nJSON saved successfully to Resume_data_pdf_1.json\nCompleted processing /kaggle/input/testlres/cover letter.pdf\n\nProcessing resume 2: /kaggle/input/testlres/BHARATH_RESUME-DS_final.pdf\n```json\n{\n  \"ContactInfo\": {\n    \"Name\": \"Bharathkumar Parunandula\",\n    \"Email\": \"bharathkumar1011@gmail.com\",\n    \"Phone\": \"+91 8639078566\",\n    \"Website\": \"linkedin.com/in/bharathkumar-parunandula-23021bb1\",\n    \"Portfolio\": \"github.com/Bharathkumar1011\",\n    \"GitHub\": \"github.com/Bharathkumar1011\"\n  },\n  \"Education\": {\n    \"InstitutionName\": \"INSTITUTE OF AERONAUTICAL ENGINEERING\",\n    \"Degree\": \"Bachelor of Engineering - Aeronautical Engineering\",\n    \"GraduationDate\": null\n  },\n  \"Experience\": [\n    {\n      \"JobTitle\": \"AI/ML Engineer | NLP & Data Science Specialist\",\n      \"CompanyName\": null,\n      \"Location\": null,\n      \"DatesOfEmployment\": null,\n      \"Description\": \"Results-driven engineer with expertise in building AI-powered solutions using natural languageprocessing(NLP),machinelearning,andlargelanguagemodels(LLMs). Proven abilitytodesignanddeployscalablesystemsfordataextraction,semanticsearch,andpre dictive analytics.\"\n    }\n  ],\n  \"Skills\": {\n    \"Languages\": [\"Python\", \"NumPy\", \"Pandas\", \"Scikit-learn\", \"Matplotlib\", \"Seaborn\"],\n    \"Technologies\": [\"AI/ML\", \"NLP\", \"Machine Learning\", \"Hugging Face Transformers\", \"LangChain\", \"Llama 3\", \"SpaCy\", \"Regex\", \"Web Development\", \"Deployment\", \"Streamlit\", \"Flask\", \"Git\", \"GitHub\", \"Jupyter\", \"Google Colab\", \"Kaggle\", \"VS Code\", \"FAISS\", \"Flashrank\"],\n    \"DataHandling\": [\"CSV\", \"JSON\", \"API integration\", \"Data Wrangling\", \"Feature Engineering\", \"Data Visualization\", \"Prompt Engineering\"],\n    \"Tools\": [\"RAG-POWERED CANDIDATE RETRIEVAL SYSTEM WITH CONTEXTUAL RERANKING\"]\n  }\n}\n```\nJSON saved successfully to Resume_data_pdf_2.json\nCompleted processing /kaggle/input/testlres/BHARATH_RESUME-DS_final.pdf\n\nProcessing resume 3: /kaggle/input/testlres/BHARATH-RESUME V3.pdf\n{\n  \"Contact Information\": {\n    \"Name\": \"Bharathkumar Parunandula\",\n    \"Email\": \"bharathkumar1011@gmail.com\",\n    \"Phone Number\": \"+91 8639078566\",\n    \"Website/Portfolio/LinkedIn\": \"https://linkedin.com/in/bharathkumar-parunandula-23021bb1\",\n    \"Github Profile\": \"https://github.com/Bharathkumar1011\"\n  },\n  \"Education\": [\n    {\n      \"Institution Name\": \"INSTITUTE OF AERONAUTICAL ENGINEERING\",\n      \"Degree\": \"Bachelor of Engineering\",\n      \"Graduation Date\": null\n    }\n  ],\n  \"Experience\": [\n    {\n\n      \"Job Title\": \"AI/ML Engineer | NLP & Data Science Specialist\",\n      \"Company Name\": \"null\",\n      \"Location\": \"Hyderabad, India\",\n      \"Dates of Employment\": null,\n      \"Description\": \"Results-driven engineer with expertise in building AI-powered solutions using natural language processing (NLP), machine learning, and large language models (LLMs).\\nProven ability to design and deploy scalable systems for data extraction, semantic search, and predictive analytics.\"\n    },\n    {\n\n      \"Job Title\": null,\n      \"Company Name\": null,\n      \"Location\": null,\n      \"Dates of Employment\": null,\n      \"Description\": \"Developed AI-powered resume search system using NLP and vector databases to match 1,000+ resumes with complex job requirements.\\nImplemented FAISS vector storage with ‘gte-small‘ embeddings for efficient cosine similarity search.\\nBoosted precision by 40% using Flashrank reranking, surfacing optimal candidates from initial retrievals through contextual reordering.\"\n    },\n    {\n\n      \"Job Title\": null,\n      \"Company Name\": null,\n      \"Location\": null,\n      \"Dates of Employment\": null,\n      \"Description\": \"Developed a Streamlit web app that parses resumes (PDFs) into structured JSON data using Groq’s LLaMA 3-70B and LangChain.\"\n    },\n    {\n\n      \"Job Title\": null,\n      \"Company Name\": null,\n      \"Location\": null,\n      \"Dates of Employment\": null,\n      \"Description\": \"Parsed nested JSON resumes with prioritized key extraction & case-insensitive normalization.\\nDeveloped job description matching with configurable embedding models (MiniLM variants) for skills, experience, and education scoring.\"\n    },\n    {\n\n      \"Job Title\": null,\n      \"Company Name\": null,\n      \"Location\": null,\n      \"Dates of Employment\": null,\n      \"Description\": \"Developed an AI-driven tool using Groq’s Llama 3-70B model to analyze resume-JD alignment, generating structured feedback on key strengths, missing skills, and improvement areas.\"\n    }\n  ],\n  \"Skills\": {\n    \"Technical Skills\": [\n      \"Python\",\n      \"Pandas\",\n      \"Scikit-learn\",\n      \"Hugging Face\",\n      \"LangChain\",\n      \"SpaCy\",\n      \"Regex\",\n      \"Streamlit\",\n      \"Flask\",\n      \"Git\",\n      \"AI Resume Parser & Structured Data Extraction Tool\",\n      \"FAISS\",\n      \"Flashrank\",\n      \"Jupyter\",\n      \"Colab\",\n      \"Kaggle\",\n      \"VS Code\"\n    ]\n  }\n}\nJSON saved successfully to Resume_data_pdf_3.json\nCompleted processing /kaggle/input/testlres/BHARATH-RESUME V3.pdf\n\nProcessing resume 4: /kaggle/input/testlres/BHARATH_RESUME-DS-FINAL.pdf\n```json\n{\n  \"contactInformation\": {\n    \"name\": \"Bharathkumar Parunandula\",\n    \"email\": \"bharathkumar1011@gmail.com\",\n    \"phoneNumber\": \"+91 8639078566\",\n    \"website\": [\n      \"linkedin.com/in/bharathkumar-parunandula-23021bb1\",\n      \"github.com/Bharathkumar1011\",\n      \"kaggle.com/work/collections\"\n    ],\n    \"githubProfile\": \"github.com/Bharathkumar1011\"\n  },\n  \"education\": {\n    \"institution\": \"INSTITUTE OF AERONAUTICAL ENGINEERING\",\n    \"degree\": \"Bachelor of Engineering - Aeronautical Engineering\",\n    \"graduationDate\": null\n  },\n  \"experience\": [\n    {\n      \"jobTitle\": null,\n      \"companyName\": null,\n      \"location\": null,\n      \"datesOfEmployment\": null,\n      \"description\": \"Developed AI-powered resume search system using NLP and vector databases to match 1,000+ resumes with complex job requirements.\"\n    },\n    {\n      \"jobTitle\": \"AI Resume Parser & Structured Data Extraction Tool\",\n      \"companyName\": null,\n      \"location\": null,\n      \"datesOfEmployment\": null,\n      \"description\": \"Developed a Streamlit web app that parses resumes (PDFs) into structured JSON data using Groq's LLaMA 3-70B and LangChain.\"\n    },\n    {\n      \"jobTitle\": \"Resume Scoring Engine: Match candidates with job descriptions\",\n      \"companyName\": null,\n      \"location\": null,\n      \"datesOfEmployment\": null,\n      \"description\": \"Parsed nested JSON resumes with prioritized key extraction & normalization, matched job descriptions using MiniLM-based embeddings for skill/edu/exp scoring.\"\n    },\n    {\n      \"jobTitle\": \"AI-Powered Resume Feedback & Skill Gap Analysis System\",\n      \"companyName\": null,\n      \"location\": null,\n      \"datesOfEmployment\": null,\n      \"description\": \"Analyzed resume-JD alignment with detailed feedback using Groq's Llama 3-70B.\"\n    }\n  ],\n  \"skills\": [\n    \"AI/ML Engineer\",\n    \"NLP & Data Science Specialist\",\n    \"Python\",\n    \"NumPy\",\n    \"Pandas\",\n    \"Scikit-learn\",\n    \"Matplotlib\",\n    \"Seaborn\",\n    \"Machine Learning & NLP\",\n    \"Hugging Face Transformers\",\n    \"LangChain\",\n    \"Llama 3\",\n    \"SpaCy\",\n    \"Regex\",\n    \"Web & Deployment\",\n    \"Streamlit\",\n    \"Flask\",\n    \"Git\",\n    \"GitHub\",\n    \"Data Handling\",\n    \"CSV\",\n    \"JSON\",\n    \"API integration\",\n    \"Tools & Platforms\",\n    \"Jupyter\",\n    \"Google Colab\",\n    \"Kaggle\",\n    \"Certifications\",\n    \"Python for Everybody - Coursera\",\n    \"Python Data Structures\",\n    \"FAISS\",\n    \"Flashrank\",\n    \"Data Wrangling\",\n    \"Feature Engineering\",\n    \"Data Visualization\",\n    \"Prompt Engineering\"\n  ]\n}\n```\nJSON saved successfully to Resume_data_pdf_4.json\nCompleted processing /kaggle/input/testlres/BHARATH_RESUME-DS-FINAL.pdf\n\nProcessing resume 5: /kaggle/input/testlres/Bharathkumar-Parunandula-Resume.pdf\n```json\n{\n    \"contactInformation\": {\n        \"name\": \"Bharathkumar Parunandula\",\n        \"email\": \"bharathkumar1011@gmail.com\",\n        \"phoneNumber\": \"86193578566\",\n        \"website\": \"https://www.kaggle.com/work/collections\",\n        \"linkedin\": \"linkedin.com/in/bharathkumar-parunandula-23021bb1\",\n        \"github\": \"https://github.com/Bharathkumar1011\"\n    },\n    \"education\": [\n        {\n            \"institutionName\": \"INSTITUTE OF AERONAUTICAL ENGINEERING\",\n            \"degree\": \"Bachelor of Engineering\",\n            \"graduationDate\": null,\n            \"major\": \"Aeronautical Engineering\"\n        }\n    ],\n    \"experience\": [\n        {\n            \"jobTitle\": null,\n            \"companyName\": null,\n            \"location\": \"Hyderabad, India\",\n            \"datesOfEmployment\": null,\n            \"description\": \"AI/ML Engineer | NLP & Data Science Specialist\\nResults-driven engineer with expertise in building AI-powered solutions using natural language\\nprocessing (NLP), machine learning, and large language models (LLMs).\"\n        }\n    ],\n    \"projects\": [\n        {\n            \"name\": \"RAG-Powered Candidate Retrieval System with Contextual Reranking\",\n            \"description\": \"Developed AI-powered resume search system using NLP and vector databases to match 1,000+ resumes with complex job requirements.\",\n            \"techStack\": [\"Python\", \"Hugging Face Transformers\", \"LangChain\", \"FAISS\", \"Flashrank\"]\n        },\n        {\n            \"name\": \"AI Resume Parser & Structured Data Extraction Tool\",\n            \"description\": \"Automated PDF-to-JSON Converter for ATS Integration\\nDeveloped a Streamlit web app that parses resumes (PDFs) into structured JSON data using\\nGroq’s LLaMA 3-70B and LangChain.\",\n            \"techStack\": [\"Python\", \"Streamlit\", \"Groq API\", \"LangChain\", \"PDFPlumber\"]\n        },\n        {\n            \"name\": \"Resume Scoring Engine: Match Candidates with Job Descriptions\",\n            \"description\": \"Parsed nested JSON resumes with prioritized key extraction & case-insensitive normalization\\nDeveloped job description matching with configurable embedding models (MiniLM variants) for skills, experience, and education scoring\",\n            \"techStack\": [\"Python\", \"Streamlit\", \"Sentence-Transformers\", \"Matplotlib\"]\n        },\n        {\n            \"name\": \"AI-Powered Resume Feedback & Skill Gap Analysis System\",\n            \"description\": \"Developed an AI-driven tool using Groq’s Llama 3-70B model to analyze resume-JD alignment, generating structured feedback on key strengths, missing skills, and improvement areas.\",\n            \"techStack\": [\"Python\", \"Streamlit\", \"Groq API\", \"Llama 3-70B\"]\n        }\n    ],\n    \"skills\": {\n        \"languagesAndLibraries\": [\"Python\", \"NumPy\", \"Pandas\", \"Scikit-learn\", \"Matplotlib\", \"Seaborn\"],\n        \"machineLearningAndNLP\": [\"Hugging Face Transformers\", \"LangChain\", \"Llama 3\", \"SpaCy\", \"Regex\"],\n        \"webAndDeployment\": [\"Streamlit\", \"Flask\", \"Git\", \"GitHub\"],\n        \"dataHandling\": [\"CSV\", \"JSON\", \"API integration\"],\n        \"toolsAndPlatforms\": [\"Jupyter\", \"Google Colab\", \"Kaggle Notebooks\", \"Visual Studio Code\"],\n        \"other\": [\"Data Wrangling\", \"Feature Engineering\", \"Data Visualization\", \"Prompt Engineering\"]\n    },\n    \"certifications\": {\n        \"name\": \"Python for Everybody\",\n        \"description\": \"Coursera Specialization\",\n        \"course\": [\"Python Data Structures\", \"Using Python to Access Web Data\", \"Using Databases with Python\"]\n    }\n}\n```\nJSON saved successfully to Resume_data_pdf_5.json\nCompleted processing /kaggle/input/testlres/Bharathkumar-Parunandula-Resume.pdf\n\nProcessing resume 6: /kaggle/input/testlres/Bharathkumar-Parunandula-FlowCV-Resume-20250704.pdf\n{\n  \"name\": \"Bharathkumar Parunandula\",\n  \"contactInformation\": {\n    \"email\": \"bharathkumar1011@gmail.com\",\n    \"phoneNumber\": \"+918639078566\",\n    \"website\": [\"linkedin.com/in/bharathkumar-parunandula-23021bb1\", \"github.com/Bharathkumar1011\", \"https://www.kaggle.com/work/collections\"],\n    \"githubProfile\": \"github.com/Bharathkumar1011\"\n  },\n  \"education\": {\n    \"institutionName\": \"INSTITUTE OF AERONAUTICAL ENGINEERING\",\n    \"degree\": \"Bachelor of Engineering\",\n    \"graduationDate\": \"null\",\n    \"fieldOfStudy\": \"Aeronautical Engineering\"\n  },\n  \"experience\": null,\n  \"projects\": [\n    {\n      \"title\": \"RAG-Powered Candidate Retrieval System with Contextual Reranking\",\n      \"description\": \"Developed AI-powered resume search system using NLP and vector databases to match 1,000+ resumes with complex job requirements.\",\n      \"techStack\": [\"Python\", \"Hugging Face Transformers\", \"LangChain\", \"FAISS\", \"Flashrank\"]\n    },\n    {\n      \"title\": \"AI Resume Parser & Structured Data Extraction Tool\",\n      \"description\": \"Automated PDF-to-JSON Converter for ATS Integration\",\n      \"techStack\": [\"Python\", \"Streamlit\", \"Groq’s LLaMA 3-70B\", \"LangChain\", \"PDFPlumber\"]\n    },\n    {\n      \"title\": \"Resume Scoring Engine: Match Candidates with Job Descriptions\",\n      \"description\": \"Parsed nested JSON resumes with prioritized key extraction & case-insensitive normalization\",\n      \"techStack\": [\"Python\", \"Streamlit\", \"Sentence-Transformers\", \"Matplotlib\"]\n    },\n    {\n      \"title\": \"AI-Powered Resume Feedback & Skill Gap Analysis System\",\n      \"description\": \"Developed an AI-driven tool using Groq’s Llama 3-70B model to analyze resume-JD alignment\",\n      \"techStack\": [\"Python\", \"Streamlit\", \"Groq API\", \"Llama 3-70B\"]\n    }\n  ],\n  \"skills\": [\"Python\", \"NumPy\", \"Pandas\", \"Scikit-learn\", \"Matplotlib\", \"Seaborn\", \"Hugging Face Transformers\", \"LangChain\", \"Llama 3\", \"SpaCy\", \"Regex\", \"Streamlit\", \"Flask\", \"Git\", \"GitHub\", \"Data Wrangling\", \"Feature Engineering\", \"Data Visualization\", \"Prompt Engineering\"]\n}\nJSON saved successfully to Resume_data_pdf_6.json\nCompleted processing /kaggle/input/testlres/Bharathkumar-Parunandula-FlowCV-Resume-20250704.pdf\n\nProcessing resume 7: /kaggle/input/testlres/BHARATH_RESUME-DS-3.pdf\n```json\n{\n  \"Contact Information\": {\n    \"Name\": \"Bharathkumar Parunandula\",\n    \"Email\": \"bharathkumar1011@gmail.com\",\n    \"Phone Number\": \"+91 8639078566\",\n    \"Website/Portfolio/LinkedIn\": \"linkedin.com/in/bharathkumar-parunandula-23021bb1\",\n    \"Github Profile\": \"github.com/Bharathkumar1011\"\n  },\n  \"Education\": [\n    {\n      \"Institution Name\": \"INSTITUTE OF AERONAUTICAL ENGINEERING\",\n      \"Degree\": \"Bachelor of Engineering - Aeronautical Engineering\",\n      \"Graduation Date\": null\n    }\n  ],\n  \"Experience\": [\n    {\n      \"Job Title\": null,\n      \"Company Name\": null,\n      \"Location\": null,\n      \"Dates of Employment\": null,\n      \"Description\": null\n    }\n  ],\n  \"Skills\": {\n    \"Languages & Libraries\": [\n      \"Python\",\n      \"NumPy\",\n      \"Pandas\",\n      \"Scikit-learn\",\n      \"Matplotlib\",\n      \"Seaborn\"\n    ],\n    \"Machine Learning & NLP\": [\n      \"Hugging Face Transformers\",\n      \"LangChain\",\n      \"Llama 3\",\n      \"SpaCy\",\n      \"Regex\"\n    ],\n    \"Web & Deployment\": [\n      \"Streamlit\",\n      \"Flask\",\n      \"Git\",\n      \"GitHub\"\n    ],\n    \"Data Handling\": [\n      \"CSV\",\n      \"JSON\",\n      \"API integration\"\n    ],\n    \"Tools & Platforms\": [\n      \"Jupyter\",\n      \"Google Colab\",\n      \"Kaggle\",\n      \"VS Code\"\n    ],\n    \"Other\": [\n      \"Data Wrangling\",\n      \"Feature Engineering\",\n      \"Data Visualization\",\n      \"Prompt Engineering\"\n    ]\n  }\n}\n```\nJSON saved successfully to Resume_data_pdf_7.json\nCompleted processing /kaggle/input/testlres/BHARATH_RESUME-DS-3.pdf\n\nProcessing resume 8: /kaggle/input/testlres/Bharathkumar-Parunandula-resume-final.pdf\n```json\n{\n  \"Contact Information\": {\n    \"Name\": \"Bharathkumar Parunandula\",\n    \"Email\": \"bharathkumar1011@gmail.com\",\n    \"Phone Number\": \"+918639078566\",\n    \"Website/Portfolio/LinkedIn\": {\n      \"LinkedIn\": \"linkedin.com/in/bharathkumar-parunandula-23021bb1\",\n      \"Portfolio\": null,\n      \"Website\": null\n    },\n    \"Github Profile\": \"github.com/Bharathkumar1011\"\n  },\n  \"Education\": {\n    \"Institution Name\": \"INSTITUTE OF AERONAUTICAL ENGINEERING\",\n    \"Degree\": \"Bachelor of Engineering - Aeronautical Engineering\",\n    \"Graduation Date\": null\n  },\n  \"Experience\": [\n    {\n      \"Job Title\": \"AI/ML Engineer | NLP & Data Science Specialist\",\n      \"Company Name\": \"N/A\",\n      \"Location\": null,\n      \"Dates of Employment\": null,\n      \"Description\": \"Results-driven engineer with expertise in building AI-powered solutions using natural language processing (NLP), machine learning, and large language models (LLMs).\"\n    }\n  ],\n  \"Skills\": {\n    \"Programming Languages & Libraries\": [\"Python\", \"NumPy\", \"Pandas\", \"Scikit-learn\", \"Matplotlib\", \"Seaborn\"],\n    \"Machine Learning & NLP\": [\"Hugging Face Transformers\", \"LangChain\", \"Llama 3\", \"SpaCy\", \"Regex\"],\n    \"Web & Deployment\": [\"Streamlit\", \"Flask\", \"Git\", \"GitHub\"],\n    \"Data Handling\": [\"CSV\", \"JSON\", \"API integration\"],\n    \"Tools & Platforms\": [\"Jupyter\", \"Google Colab\", \"Kaggle Notebooks\", \"Visual Studio Code\"],\n    \"Other\": [\"Data Wrangling\", \"Feature Engineering\", \"Data Visualization\", \"Prompt Engineering\"]\n  },\n  \"Certifications\": [\n    {\n      \"Name\": \"Python for Everybody\",\n      \"Course Provider\": \"Coursera\",\n      \"Completion Status\": \"Completed\"\n    },\n    {\n      \"Name\": \"Python Data Structures\",\n      \"Course Provider\": \"Coursera\",\n      \"Completion Status\": \"Completed\"\n    },\n    {\n      \"Name\": \"Using Python to Access Web Data\",\n      \"Course Provider\": \"Coursera\",\n      \"Completion Status\": \"Completed\"\n    },\n    {\n      \"Name\": \"Using Databases with Python\",\n      \"Course Provider\": \"Coursera\",\n      \"Completion Status\": \"Completed\"\n    }\n  ]\n}\n```\nJSON saved successfully to Resume_data_pdf_8.json\nCompleted processing /kaggle/input/testlres/Bharathkumar-Parunandula-resume-final.pdf\n\nProcessing resume 9: /kaggle/input/testlres/Bharathkumar-Parunandula-FlowCV-Resume-20250619.pdf\n```json\n{\n  \"Contact Information\": {\n    \"Name\": \"Bharathkumar Parunandula\",\n    \"Email\": \"bharathkumar1011@gmail.com\",\n    \"Phone Number\": \"+918639078566\",\n    \"Website/Portfolio/LinkedIn\": \"linkedin.com/in/bharathkumar-parunandula-23021bb1\",\n    \"Github Profile\": \"github.com/Bharathkumar1011\"\n  },\n  \"Education\": {\n    \"Institution Name\": \"INSTITUTE OF AERONAUTICAL ENGINEERING\",\n    \"Degree\": \"Bachelor of Engineering - Aeronautical Engineering\",\n    \"Graduation Date\": null\n  },\n  \"Experience\": {\n    \"Job Title\": null,\n    \"Company Name\": null,\n    \"Location\": null,\n    \"Dates of Employment\": null,\n    \"Description\": null\n  },\n  \"Projects\": {\n    \"Resume Parser & Ranker (NLP + LLM)\": {\n      \"Description\": \"Built an end-to-end system to parse, extract, and rank resumes using NLP techniques.\",\n      \"Key Features\": [\n        \"Integrated spaCy, regex, and Hugging Face transformers (LLaMA) for semantic extraction.\",\n        \"Implemented ranking logic using keyword matching, section weightage, and similarity scoring.\",\n        \"Visualized results using Matplotlib; modular design enables easy extension and automation.\"\n      ]\n    },\n    \"Ranker Streamlit App (Frontend for Resume Parsing)\": {\n      \"Description\": \"Developed a UI interface for resume ranking results; built with Streamlit.\",\n      \"Key Features\": [\n        \"Displays parsed content, ranking scores, and visual charts.\"\n      ]\n    },\n    \"Feedback Collection App (Streamlit)\": {\n      \"Description\": \"Created a tool for collecting and analyzing user feedback via CSV uploads.\",\n      \"Key Features\": [\n        \"Generates dynamic charts and insights using Matplotlib and Pandas.\"\n      ]\n    },\n    \"Kaggle Notebook Portfolio\": {\n      \"Description\": \"Created 100+ public notebooks covering classification, regression, NLP, and data cleaning.\",\n      \"Key Features\": [\n        \"Includes complete ML pipelines, visual storytelling, and real-world datasets.\"\n      ]\n    }\n  },\n  \"Skills\": {\n    \"Languages & Libraries\": [\"Python\", \"NumPy\", \"Pandas\", \"Scikit-learn\", \"Matplotlib\", \"Seaborn\"],\n    \"Machine Learning & NLP\": [\"Llama 3\", \"Hugging Face Transformers\", \"SpaCy\", \"Regex\"],\n    \"Web & Deployment\": [\"Streamlit\", \"Flask\", \"Git\", \"GitHub\"],\n    \"Data Handling\": [\"CSV\", \"JSON\", \"API integration\"],\n    \"Tools & Platforms\": [\"Jupyter\", \"Google Colab\", \"Kaggle Notebooks\", \"Git\"],\n    \"Other\": [\"Data Wrangling\", \"Feature Engineering\", \"Data Visualization\", \"Prompt Engineering\"]\n  },\n  \"Certifications\": {\n    \"Name\": \"Python for Everybody\",\n    \"Provider\": \"Coursera Specialization\",\n    \"Link\": \"coursera.org/account/accomplishments/specialization/LMST7ZT2F8LD\"\n  }\n}\n```\nJSON saved successfully to Resume_data_pdf_9.json\nCompleted processing /kaggle/input/testlres/Bharathkumar-Parunandula-FlowCV-Resume-20250619.pdf\n\nProcessing resume 10: /kaggle/input/testlres/BHARATH-RESUME-DS.pdf\n```json\n{\n  \"Contact Information\": {\n    \"Name\": \"Bharathkumar Parunandula\",\n    \"Email\": \"bharathkumar1011@gmail.com\",\n    \"Phone Number\": null,\n    \"Website/Portfolio/LinkedIn\": [\"linkedin.com/in/bharathkumar-parunandula-23021bb1\"],\n    \"Github Profile\": [\"github.com/Bharathkumar1011\"]\n  },\n  \"Education\": {\n    \"Institution Name\": \"INSTITUTE OF AERONAUTICAL ENGINEERING\",\n    \"Degree\": \"Bachelor of Engineering - Aeronautical Engineering\",\n    \"Graduation Date\": null\n  },\n  \"Experience\": [\n    {\n      \"Job Title\": \"AI/ML Engineer — NLP & Data Science Specialist\",\n      \"Company Name\": null,\n      \"Location\": null,\n      \"Dates of Employment\": null,\n      \"Description\": \"Results-driven engineer with expertise in building AI-powered solutions using natural language processing(NLP),machinelearning,andlargelanguagemodels(LLMs). Proven abilitytodesignanddeployscalablesystemsfordataextraction,semanticsearch,andpre- dictive analytics.\"\n    }\n  ],\n  \"Skills\": {\n    \"Technical Skills\": [\n      \"Python\",\n      \"NumPy\",\n      \"Pandas\",\n      \"Scikit-learn\",\n      \"Matplotlib\",\n      \"Seaborn\",\n      \"Hugging Face Transformers\",\n      \"LangChain\",\n      \"Llama 3\",\n      \"SpaCy\",\n      \"Regex\",\n      \"Streamlit\",\n      \"Flask\",\n      \"Git\",\n      \"GitHub\",\n      \"Jupyter\",\n      \"Google Colab\",\n      \"Kaggle\",\n      \"Notebooks\",\n      \"VS Code\",\n      \"FAISS\",\n      \"Flashrank\",\n      \"LangChain\",\n      \"Groq API\",\n      \"LangChain\",\n      \"PDFPlumber\",\n      \"Sentence-Transformers\",\n      \"Matplotlib\"\n    ],\n    \"Tools & Platforms\": [\n      \"NLP\",\n      \"Data Science\",\n      \"CSV\",\n      \"JSON\",\n      \"API integration\",\n      \"AI/ML\",\n      \"RAG-Powered Candidate Retrieval System\",\n      \"vector databases\",\n      \"Streamlit webapp\",\n      \"LLaMA 3-70B\"\n    ]\n  }\n}\n```\n\nPlease note that I've extracted the available information as per the given instructions, but the JSON object may not perfectly mimic the original structure of the resume due to missing information like company name, location, and dates of employment.\nJSON saved successfully to Resume_data_pdf_10.json\nCompleted processing /kaggle/input/testlres/BHARATH-RESUME-DS.pdf\n\nProcessing resume 11: /kaggle/input/testlres/BHARATH_RESUME-DS2.pdf\n{\n  \"contactInformation\": {\n    \"name\": \"Bharathkumar Parunandula\",\n    \"email\": \"bharathkumar1011@gmail.com\",\n    \"phoneNumber\": \"+91 8639078566\",\n    \"website\": \"linkedin.com/in/bharathkumar-parunandula-23021bb1\",\n    \"portfolio\": null,\n    \"githubProfile\": \"github.com/Bharathkumar1011\"\n  },\n  \"education\": {\n    \"institutionName\": \"INSTITUTE OF AERONAUTICAL ENGINEERING\",\n    \"degree\": \"Bachelor of Engineering - Aeronautical Engineering\",\n    \"graduationDate\": \"1\"  // assumes \"1\" to be the graduation date, could be a typo, actual value is unknown\n  },\n  \"experience\": {},\n  \"skills\": {\n    \"skills\": [\n      \"AI/ML Engineer\",\n      \"NLP & Data Science Specialist\",\n      \"Python\",\n      \"NumPy\",\n      \"Pandas\",\n      \"Scikit-learn\",\n      \"Matplotlib\",\n      \"Seaborn\",\n      \"Hugging Face Transformers\",\n      \"LangChain\",\n      \"Llama 3\",\n      \"SpaCy\",\n      \"Regex\",\n      \"Streamlit\",\n      \"Flask\",\n      \"Git\",\n      \"GitHub\",\n      \"Jupyter\",\n      \"Google Colab\",\n      \"Kaggle\",\n      \"Notebooks\",\n      \"VS Code\"\n    ]\n  },\n  \"certifications\": {\n    \"certifications\": [\n      \"Python for Everybody - Coursera\",\n      \"Python Data Structures\",\n      \"Automated PDF-to-JSON Converter for ATS Integration\",\n      \"Using Python to Access Web Data\",\n      \"Using Databases with Python\"\n    ]\n  },\n  \"projects\": {\n    \"projects\": [\n      {\n        \"name\": \"RAG-Powered Candidate Retrieval System with Contextual Reranking\",\n        \"description\": \"Deployed AI-powered resume search system using NLP and vector database to match 1,000+ resumes with complex job requirements.\",\n        \"technologies\": [\n          \"Python\",\n          \"Hugging Face Transformers\",\n          \"LangChain\",\n          \"FAISS\",\n          \"Flashrank\"\n        ]\n      },\n      {\n        \"name\": \"AI Resume Parser & Structured Data Extraction Tool\",\n        \"description\": \"Developed Streamlit web app that parses resumes (PDFs) into structured JSON data using Groq’s LLaMA 3-70B and LangChain.\",\n        \"technologies\": [\n          \"Python\",\n          \"Streamlit\",\n          \"Groq API\",\n          \"LangChain\",\n          \"PDFPlumber\"\n        ]\n      },\n      {\n        \"name\": \"Resume Scoring Engine: Match Candidates with Job Descriptions\",\n        \"description\": \"Parsed nested JSON resumes with prioritized key extraction & normalization, matched job descriptions using MiniLM-based embeddings for skill/edu/exp scoring.\",\n        \"technologies\": [\n          \"Python\",\n          \"Streamlit\",\n          \"Sentence-Transformers\",\n          \"Matplotlib\"\n        ]\n      },\n      {\n        \"name\": \"AI-Powered Resume Feedback & Skill Gap Analysis System\",\n        \"description\": \"Developed AI tool using Groq’s Llama 3-70B to analyze resume-JD alignment with detailed feedback, implemented synonym matching, skill gap detection, and penalty scoring for core competencies.\",\n        \"technologies\": [\n          \"Python\",\n          \"Streamlit\",\n          \"Groq API\",\n          \"Llama 3-70B\"\n        ]\n      }\n    ]\n  }\n}Error parsing JSON for /kaggle/input/testlres/BHARATH_RESUME-DS2.pdf: Expecting ',' delimiter: line 13 column 28 (char 471)\nCompleted processing /kaggle/input/testlres/BHARATH_RESUME-DS2.pdf\n\nProcessing resume 12: /kaggle/input/testlres/BHARATH_RESUME-complete.pdf\n{\n  \"Contact Information\": {\n    \"Name\": \"Bharathkumar Parunandula\",\n    \"Email\": \"bharathkumar1011@gmail.com\",\n    \"Phone Number\": \"+91 8639078566\",\n    \"Website/Portfolio/LinkedIn\": \"linkedin.com/in/bharathkumar-parunandula-23021bb1\",\n    \"Github Profile\": \"github.com/Bharathkumar1011\"\n  },\n  \"Education\": {\n    \"Institution Name\": \"INSTITUTE OF AERONAUTICAL ENGINEERING, Hyderabad, India\",\n    \"Degree\": \"Bachelor of Engineering\",\n    \"Stream\": \"Aeronautical Engineering\",\n    \"Graduation Date\": \"null\"\n  },\n  \"Experience\": [\n    {\n      \"Job Title\": null,\n      \"Company Name\": null,\n      \"Location\": null,\n      \"Dates of Employment\": null,\n      \"Description\": \"null\"\n    }\n  ],\n  \"Skills\": [\n    \"Python\",\n    \"NumPy\",\n    \"Pandas\",\n    \"Scikit-learn\",\n    \"Matplotlib\",\n    \"Seaborn\",\n    \"Hugging Face Transformers\",\n    \"LangChain\",\n    \"Llama 3\",\n    \"SpaCy\",\n    \"Regex\",\n    \"Streamlit\",\n    \"Flask\",\n    \"Git\",\n    \"GitHub\",\n    \"Jupyter\",\n    \"Google Colab\",\n    \"Kaggle\",\n    \"Data Wrangling\",\n    \"Feature Engineering\",\n    \"Data Visualization\",\n    \"Prompt Engineering\",\n    \"FAISS\",\n    \"Flashrank\"\n  ]\n}\nJSON saved successfully to Resume_data_pdf_12.json\nCompleted processing /kaggle/input/testlres/BHARATH_RESUME-complete.pdf\n\nProcessing resume 13: /kaggle/input/testlres/BHARATH-RESUME V2.pdf\n```json\n{\n  \"contact\": {\n    \"name\": \"Bharathkumar Parunandula\",\n    \"email\": \"bharathku-1011@gmail.com\",\n    \"phone\": \"+91 8639078566\",\n    \"linkedin\": \"linkedin.com/in/bharathkumar-parunandula-23021bb1\",\n    \"github\": \"github.com/Bharathkumar1011\"\n  },\n  \"education\": {\n    \"institution\": \"INSTITUTE OF AERONAUTICAL ENGINEERING\",\n    \"degree\": \"Bachelor of Engineering - Aeronautical Engineering\",\n    \"graduation_date\": null\n  },\n  \"experience\": [\n    {\n      \"job_title\": \"AI/ML Engineer | NLP & Data Science Specialist\",\n      \"company_name\": null,\n      \"location\": \"Hyderabad, India\",\n      \"dates_of_employment\": null,\n      \"description\": \"Results-driven engineer with expertise in building AI-powered solutions using NLP, machine learning, and large language models. Proven ability to design and deploy scalable systems for data extraction, semantic search, and predictive analytics.\"\n    }\n  ],\n  \"projects\": {\n    \"title\": \"RAG-Powered Candidate Retrieval System with Contextual Reranking\",\n    \"description\": \"Python, Hugging Face Transformers, LangChain, FAISS, Flashrank.\",\n    \"skills\": null\n  },\n  \"skills\": {\n    \"category\": \"Technical\",\n    \"skills\": [\n      \"Python\", \"NumPy\", \"Pandas\", \"Scikit-learn\", \"Matplotlib\", \"Seaborn\", \"Hugging Face Transformers\",\n      \"LangChain\", \"FAISS\", \"Flashrank\", \"SpaCy\", \"Regex\"\n    ]\n  },\n  \"certifications\": [\n    {\n      \"title\": \"Python for Everybody - Coursera Specialization\",\n      \"description\": null,\n      \"date\": null\n    },\n    {\n      \"title\": \"Python Data Structures - Using Python to Access Web Data - Using Databases with Python\",\n      \"description\": null,\n      \"date\": null\n    }\n  ],\n  \"awards\": [\n    {\n      \"title\": \"AI-Powered Resume Feedback & Skill Gap Analysis System\",\n      \"description\": \"Python, Streamlit, Groq API, Llama 3-70B.\",\n      \"date\": null\n    }\n  ]\n}\n```\nJSON saved successfully to Resume_data_pdf_13.json\nCompleted processing /kaggle/input/testlres/BHARATH-RESUME V2.pdf\n\nProcessing resume 14: /kaggle/input/testlres/cover1.pdf\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPDFSyntaxError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pdfplumber/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stream, stream_is_external, path, pages, laparams, password, strict_metadata, unicode_norm, raise_unicode_errors)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDFDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPDFParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassword\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pdfminer/pdfdocument.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parser, password, caching, fallback)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPDFSyntaxError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No /Root object! - Is this really a PDF?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Type\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mLITERAL_CATALOG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPDFSyntaxError\u001b[0m: No /Root object! - Is this really a PDF?","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mPdfminerException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/810713488.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpdf_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nProcessing resume {i}: {pdf_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mprocess_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Completed processing {pdf_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/810713488.py\u001b[0m in \u001b[0;36mprocess_resume\u001b[0;34m(pdf_path, index)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_resume\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;31m# Extract text from PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_text_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# Call LLM API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/810713488.py\u001b[0m in \u001b[0;36mextract_text_pdf\u001b[0;34m(pdf_path)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mextract_text_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDFPlumberLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/document_loaders/pdf.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/document_loaders/base.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, blob)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \"\"\"\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/document_loaders/parsers/pdf.py\u001b[0m in \u001b[0;36mlazy_parse\u001b[0;34m(self, blob)\u001b[0m\n\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpdfplumber\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# open document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m             yield from [\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pdfplumber/pdf.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(cls, path_or_fp, pages, laparams, password, strict_metadata, unicode_norm, repair, gs_path, repair_setting, raise_unicode_errors)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             return cls(\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pdfplumber/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stream, stream_is_external, path, pages, laparams, password, strict_metadata, unicode_norm, raise_unicode_errors)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDFDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPDFParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassword\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPdfminerException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsrcmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDFResourceManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mPdfminerException\u001b[0m: No /Root object! - Is this really a PDF?"],"ename":"PdfminerException","evalue":"No /Root object! - Is this really a PDF?","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"# isuue 1 - hallusination,not proper parsing","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T15:24:50.565447Z","iopub.execute_input":"2025-07-18T15:24:50.565973Z","iopub.status.idle":"2025-07-18T15:24:50.569369Z","shell.execute_reply.started":"2025-07-18T15:24:50.565951Z","shell.execute_reply":"2025-07-18T15:24:50.568727Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}